{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f0f4f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando o dataframe\n",
    "# Primeira análise apenas com extroversão\n",
    "data_path = \"../chalearn_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c13681bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>extraversion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zEyRyTnIw5I.005.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nskJh7v6v1U.004.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eHcRre1YsNA.000.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VuadgOz6T7s.000.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7nhJXn9PI0I.001.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>9yZEb6bdxNY.004.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>dNXqs5HNijI.004.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>rG8D-A2F8xg.004.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>F-Dy1EFm_Mw.005.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>UOvgGSx9k_k.002.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Unnamed: 0  extraversion\n",
       "0     zEyRyTnIw5I.005.mp4             0\n",
       "1     nskJh7v6v1U.004.mp4             0\n",
       "2     eHcRre1YsNA.000.mp4             0\n",
       "3     VuadgOz6T7s.000.mp4             0\n",
       "4     7nhJXn9PI0I.001.mp4             0\n",
       "...                   ...           ...\n",
       "1995  9yZEb6bdxNY.004.mp4             1\n",
       "1996  dNXqs5HNijI.004.mp4             1\n",
       "1997  rG8D-A2F8xg.004.mp4             1\n",
       "1998  F-Dy1EFm_Mw.005.mp4             1\n",
       "1999  UOvgGSx9k_k.002.mp4             1\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_training = pd.read_csv(f\"{data_path}/train/extraversion_data.csv\")\n",
    "df_training = pd.DataFrame.from_dict(data_training)\n",
    "\n",
    "df_training.head(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07405537",
   "metadata": {},
   "source": [
    "## Implementando um primeiro modelo\n",
    "\n",
    "EfficientNet B0 -> Transformers -> Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce8c0e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c65b4f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " efficientnetb0 (Functional)  (None, None, None, 1280)  4049571  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,049,571\n",
      "Trainable params: 0\n",
      "Non-trainable params: 4,049,571\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Camadas iniciais do modelo:\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)) # Por enquanto o input é apenas uma imagem\n",
    "# Usa EfficientNet B0 como extratora de características da imagem que iremos processar\n",
    "features_extraction = EfficientNetB0(include_top=False, weights='imagenet')\n",
    "features_extraction.trainable = False\n",
    "features_extraction = features_extraction(inputs)\n",
    "\n",
    "model = tf.keras.Model(inputs, features_extraction)\n",
    "# model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "616db9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers - Camada final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "529b9ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importações necessárias para o funcionamento da Classe\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Extrator de Frames\n",
    "class FramesExtractor(layers.Layer):\n",
    "    '''\n",
    "    data_path = caminho do diretório no qual os vídeos estão inseridos\n",
    "    number_of_frames = número de frames a ser extraído a cada iteração\n",
    "    dim = dimensões da imagem no formato (altura, largura)\n",
    "    '''\n",
    "    def __init__(self, data_path, number_of_frames=10, dim=(224, 224)):\n",
    "        super(FramesExtractor, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.number_of_frames = number_of_frames\n",
    "        self.dim= dim\n",
    "        \n",
    "\n",
    "    def call(self, video_name):\n",
    "        if video_name != None:\n",
    "            video_path = self.data_path + str(video_name)\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            frames = []\n",
    "            length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            jump_size = int(length // self.number_of_frames)\n",
    "            for _ in range(self.number_of_frames):\n",
    "                ret, frame = cap.read()\n",
    "                frames.append(cv2.cvtColor(cv2.resize(frame, self.dim), cv2.COLOR_BGR2RGB))\n",
    "                for __ in range(jump_size - 1):\n",
    "                  cap.grab()\n",
    "            return np.array(frames)\n",
    "        else:\n",
    "            return np.array()\n",
    "    \n",
    "#     def show(self, video_name):\n",
    "#         frames = self.call(video_name)\n",
    "#         print(f\"Número de frames: {frames.shape[0]}\\nFormato dos frames: {frames.shape[1:]}\")\n",
    "\n",
    "#         # Plotando o resultado:\n",
    "#         print(\"Frames extraídos:\")\n",
    "#         fig = plt.figure(figsize=(10, 10))\n",
    "#         rows = 4\n",
    "#         columns = 4\n",
    "#         for _ in range(self.number_of_frames):\n",
    "#             fig.add_subplot(rows, columns, _ + 1)\n",
    "#             plt.imshow(frames[_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dfa011f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importações necessárias para o funcionamento da Classe\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Extrator de Frames\n",
    "class FramesExtractor(layers.Layer):\n",
    "    '''\n",
    "    data_path = caminho do diretório no qual os vídeos estão inseridos\n",
    "    number_of_frames = número de frames a ser extraído a cada iteração\n",
    "    dim = dimensões da imagem no formato (altura, largura)\n",
    "    '''\n",
    "    def __init__(self, data_path, number_of_frames=10, dim=(224, 224)):\n",
    "        super(FramesExtractor, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.number_of_frames = number_of_frames\n",
    "        self.dim= dim\n",
    "        \n",
    "\n",
    "    '''\n",
    "    videos_names = vetor com nomes de vídeos a serem precessados\n",
    "    \n",
    "    retorna um array com os frames extrapidos\n",
    "    '''\n",
    "    def call(self, videos_names):\n",
    "        batch_size = videos_names.shape[0]\n",
    "        frames = []\n",
    "        print(f\"BATCH SIZE = {batch_size}\")\n",
    "        for video_name in videos_names:\n",
    "            video_path = self.data_path + str(video_name)\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            jump_size = int(length // self.number_of_frames)\n",
    "            for _ in range(self.number_of_frames):\n",
    "                ret, frame = cap.read()\n",
    "                if frame == None:\n",
    "                    frames.append(np.empty(self.dim, self.dim, 3))\n",
    "                else:\n",
    "                    frames.append(cv2.cvtColor(cv2.resize(frame, self.dim), cv2.COLOR_BGR2RGB))\n",
    "                for __ in range(jump_size - 1):\n",
    "                  cap.grab()\n",
    "        return tf.reshape(np.array(frames), [batch_size, self.number_of_frames, self.dim[0], self.dim[1], 3])\n",
    "    \n",
    "    def show(self, videos_names):\n",
    "        frames = self.call(videos_names)\n",
    "        batch_size = frames.shape[0]\n",
    "        print(f\"Batch Size = {batch_size}\")\n",
    "        print(f\"Número de frames: {frames.shape[1]}\\nFormato dos frames: {frames.shape[2:]}\")\n",
    "\n",
    "        # Plotando o resultado:\n",
    "        print(\"Frames extraídos:\")\n",
    "        for b in range(batch_size):\n",
    "            fig = plt.figure(figsize=(10, 10))\n",
    "            rows = 4\n",
    "            columns = 4\n",
    "            for _ in range(self.number_of_frames):\n",
    "                fig.add_subplot(rows, columns, _ + 1)\n",
    "                plt.imshow(frames[b][_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8370b4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_dim):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_dim = patch_dim\n",
    "\n",
    "    def call(self, tensors):\n",
    "        batch_size = tf.shape(tensors)[0]\n",
    "        patches = tf.reshape(tensors, [batch_size, -1, patch_dim])\n",
    "        print(\"Patches shape = \", patches.shape)\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "419273c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Já temos um patch projetado linearmente, só precisamos agora fazer o embedding\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, num_patches, patch_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=patch_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = patch + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a355b2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_classifier():\n",
    "#     inputs = layers.Input(shape=input_shape)\n",
    "#     # Augment data.\n",
    "#     augmented = data_augmentation(inputs)\n",
    "#     # Create patches.\n",
    "#     patches = Patches(patch_size)(augmented)\n",
    "#     # Encode patches.\n",
    "#     encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "#     # Create multiple layers of the Transformer block.\n",
    "#     for _ in range(transformer_layers):\n",
    "#         # Layer normalization 1.\n",
    "#         x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "#         # Create a multi-head attention layer.\n",
    "#         attention_output = layers.MultiHeadAttention(\n",
    "#             num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "#         )(x1, x1)\n",
    "#         # Skip connection 1.\n",
    "#         x2 = layers.Add()([attention_output, encoded_patches])\n",
    "#         # Layer normalization 2.\n",
    "#         x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "#         # MLP.\n",
    "#         x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "#         # Skip connection 2.\n",
    "#         encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "#     # Create a [batch_size, projection_dim] tensor.\n",
    "#     representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "#     representation = layers.Flatten()(representation)\n",
    "#     representation = layers.Dropout(0.5)(representation)\n",
    "#     # Add MLP.\n",
    "#     features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "#     # Classify outputs.\n",
    "#     logits = layers.Dense(num_classes)(features)\n",
    "#     # Create the Keras model.\n",
    "#     model = keras.Model(inputs=inputs, outputs=logits)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3fd3c1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patches shape =  (None, None, 64)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m patches \u001b[38;5;241m=\u001b[39m Patches(patch_dim)(features_extraction)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Encodding\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m encoded_patches \u001b[38;5;241m=\u001b[39m \u001b[43mEncoder\u001b[49m(num_patches, patch_dim)(patches)\n\u001b[1;32m     21\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel(inputs, encoded_patches)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Encoder' is not defined"
     ]
    }
   ],
   "source": [
    "# Camadas iniciais do modelo:\n",
    "patch_dim = 64\n",
    "num_patches = 1280 // patch_dim\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)) # Por enquanto o input é apenas uma imagem\n",
    "# Usa EfficientNet B0 como extratora de características da imagem que iremos processar\n",
    "features_extraction = EfficientNetB0(include_top=False, weights='imagenet')\n",
    "features_extraction.trainable = False\n",
    "features_extraction = features_extraction(inputs)\n",
    "# Patches\n",
    "patches = Patches(patch_dim)(features_extraction)\n",
    "\n",
    "# Encodding\n",
    "encoded_patches = Encoder(num_patches, patch_dim)(patches)\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs, encoded_patches)\n",
    "# model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "06dad11f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH SIZE = None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling layer \"frames_extractor_23\" (type FramesExtractor).\n\nin user code:\n\n    File \"/tmp/ipykernel_682342/1876474392.py\", line 39, in call  *\n        frames.append(np.empty(self.dim, self.dim, 3))\n\n    TypeError: Cannot interpret '224' as a data type\n\n\nCall arguments received:\n  • videos_names=tf.Tensor(shape=(None,), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [111]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Extrator de frames\u001b[39;00m\n\u001b[1;32m     11\u001b[0m video_data_path \u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/train/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m frames_extractor \u001b[38;5;241m=\u001b[39m \u001b[43mFramesExtractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# frames_extractor(inputs)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Aplica EfficientNetB0 aos frames como extratora de features\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# features_extractor = layers.TimeDistributed( efficientnet, input_shape = (10, 224, 224, 3))\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# features_extractor = features_extractor(frames_extractor)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel(inputs, frames_extractor)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:692\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 692\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m    693\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling layer \"frames_extractor_23\" (type FramesExtractor).\n\nin user code:\n\n    File \"/tmp/ipykernel_682342/1876474392.py\", line 39, in call  *\n        frames.append(np.empty(self.dim, self.dim, 3))\n\n    TypeError: Cannot interpret '224' as a data type\n\n\nCall arguments received:\n  • videos_names=tf.Tensor(shape=(None,), dtype=float32)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "# Camada de input\n",
    "input_shape = ()\n",
    "inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "# Extrator de frames\n",
    "video_data_path =f\"{data_path}/train/\"\n",
    "frames_extractor = FramesExtractor(video_data_path, number_of_frames=10, dim=(IMG_SIZE, IMG_SIZE))(inputs)\n",
    "# frames_extractor(inputs)\n",
    "\n",
    "# Aplica EfficientNetB0 aos frames como extratora de features\n",
    "# efficientnet = EfficientNetB0(include_top=False, weights='imagenet')\n",
    "# efficientnet.trainable = False\n",
    "# features_extractor = layers.TimeDistributed( efficientnet, input_shape = (10, 224, 224, 3))\n",
    "# features_extractor = features_extractor(frames_extractor)\n",
    "model = tf.keras.Model(inputs, frames_extractor)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "932f453f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "teste = np.empty((10, 224, 224, 3))\n",
    "teste = tf.reshape(teste, (-1, 10, 224, 224, 3))\n",
    "\n",
    "print(teste.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
